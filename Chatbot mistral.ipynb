{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_histroy_tempalte(memory):\n",
    "    chat_history=\"\"\n",
    "    for i in range(0,len(memory.chat_memory.messages),2):\n",
    "        human=\"Human: \"+memory.chat_memory.messages[i].content\n",
    "        ai=\"AI: \"+memory.chat_memory.messages[i+1].content\n",
    "        conversation=human+\"\\n\"+ai+\"\\n\\n\"\n",
    "        chat_history=chat_history+conversation\n",
    "    return chat_history\n",
    "    \n",
    "# Save db\n",
    "def save_db(db):\n",
    "  with open(\"faiss_index.pkl\", \"wb\") as f:\n",
    "      pickle.dump(db, f)\n",
    "\n",
    "# Load db\n",
    "def load_db():\n",
    "  with open(\"faiss_index.pkl\", \"rb\") as f:\n",
    "      loaded_db = pickle.load(f)\n",
    "      return loaded_db\n",
    "  \n",
    "def get_completion(question: str,  context: str,chat_history: str, model, tokenizer) -> str:\n",
    "  device = \"cuda:0\"\n",
    "\n",
    "  prompt_template = \"\"\"\n",
    "  <s>\n",
    "  [INST]\n",
    "    Answer the question based only on the following context:\n",
    "    {context}\n",
    "    \n",
    "    Chat_history\n",
    "    {chat_history}\n",
    "    Question: {question}\n",
    "  [/INST]\n",
    "  </s>\n",
    "  <s>\n",
    "\n",
    "  \"\"\"\n",
    "  prompt = prompt_template.format(question=question,context=context,chat_history=chat_history)\n",
    "  encodeds = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=True)\n",
    "  model_inputs = encodeds.to(device)\n",
    "\n",
    "  generated_ids = model.generate(**model_inputs, max_new_tokens=1000, do_sample=True, pad_token_id=tokenizer.eos_token_id)\n",
    "  decoded = tokenizer.batch_decode(generated_ids)\n",
    "  return (decoded[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map={\"\":0})\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, add_eos_token=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "try:\n",
    "    db=load_db()\n",
    "except:\n",
    "    loader = PyPDFLoader(\"path\\\\to\\\\your\\\\pdf\")\n",
    "    pages = loader.load_and_split()\n",
    "\n",
    "    db = FAISS.from_documents(pages, \n",
    "                            HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2'))\n",
    "    save_db(db)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect query to FAISS index using a retriever\n",
    "retriever = db.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={'k': 4}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "# Instantiate ConversationBufferMemory\n",
    "memory = ConversationBufferMemory(\n",
    " return_messages=True, output_key=\"answer\", input_key=\"question\"\n",
    ")\n",
    "inputs = {\"question\": \"My name is Optimus Prime can you remember it\"}\n",
    "memory.save_context(inputs, {\"answer\": \"of course I can remember\"})\n",
    "\n",
    "chat_history= chat_histroy_tempalte(memory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    question = str(input(\"You: \"))\n",
    "    if question.lower() in [\"quit\",\"exit\"] :\n",
    "       break\n",
    "    context = retriever.get_relevant_documents(question)\n",
    "    all_context=''\n",
    "    for i in context:\n",
    "        all_context =  all_context + i.page_content\n",
    "    result = get_completion(question=question,context=all_context, chat_history=chat_history, model=model, tokenizer=tokenizer)\n",
    "    result=result.split(\"</s>\")\n",
    "    print(\"AI: \",result[-2])\n",
    "    memory.save_context({'question':question},{'answer':result[-2]})\n",
    "    chat_history=chat_histroy_tempalte(memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
